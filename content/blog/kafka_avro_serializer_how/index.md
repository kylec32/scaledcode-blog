---
title: How the Kafka Avro Serializer Works
description: Deep dive into how the Confluent Schema Registry and Kafka collaborate in order to use Avro in Kafka.
date: 2022-02-14
hero_image: ./kafka_serialization_hero.jpg
tags:
  - avro
  - kafka
  - schema
  - design patterns
---

In a [previous post](/blog/understanding-avro-compatibility) the different levels of compatibility that the [Avro data serialization system](https://avro.apache.org/) can facilitate was discussed. Understanding the different options helps us make the right determination between the tradeoff between flexibility in schema evolution and the compatibility our data will enjoy. To fully realize the schema evolution benefits this compatibility gives you at deserialization time you need to know the schema that was used when writing the Avro data. There are many ways to accomplish this but one of the most common in the Kafka community is to use the confluent schema registry. Let's dig into how this is accomplished and along the way learn some about serialization/deserialization in Kafka, how Avro represents data, and how the Confluent schema registry works.

The first step in understanding how Avro data can be sent via Kafka is understanding how any data is sent inside of Kafka. [Kafka](https://kafka.apache.org/) is a high-performance event streaming platform. One of the reasons that Kafka can be so fast in my opinion is that it is very good at understanding where its sphere of influence begins and ends. At the core of Kafka is the concept of a topic. A topic is a named abstraction of an append-only log. You append records to this log and consumers of the topic can read from the log. What makes up a record is irrelevant to Kafka, all it deals in are raw bytes. This is where our serialization journey begins. When using a Kafka producer, one of the things you need to provide is a class that can take in a piece of data and turn that into a byte array. On the consuming side of the topic, you need to provide a class that can take that byte array and produces an object that is understandable by the runtime. A very simple pattern but a powerful one.

We are off to a good start. Kafka needs a system that can convert between data and bytes, the main capability of Avro is to do exactly that. If you never wanted to read the data that was put into Kafka we wouldn't have to go any farther. First, you would take the object that you wanted to send on Kafka, pass it through the Avro serializer, and send it on its way. This, however, wouldn't be very useful, we need to still be able to read the data and as mentioned above we need to know the schema that was used to write a piece of data if we want to read it in Avro. Let's consider some options.

The first would be to always use the same schema and put it in both the producer and consumer code. This undoubtedly is simple but it doesn't allow for any schema evolution which is limiting. Taking that into account we could choose to send the schema along with every message. Again this could be made to work however it is extremely inefficient and negates much of the benefit of using a fast protocol like Avro in the first place. We also won't be changing schemas that often and will be sending many, many messages with the same schema so we want to be able to skip that retransmission. We could choose to provide all versions of a schema to both producers and consumers so they have all possible options and they don't need to retransmit them with each message. This saves us the retransmission but it introduces the question of how would a consumer know which version of the schema was used when serializing a particular record. It also has the issue of needing to update both producers and consumers at the same time which is not practical.

None of these options get us what we want but the last option isn't too far off our final destination. This is where the schema registry comes in. The purpose of the schema registry is to be the authoritative source of a schema. While the producer and consumer can, and likely will, have their own local version of the schema as part of their executable, the schema registry serves as a common language by which producers and consumers can publish, retrieve, and check schemas between each other. At a high level, the schema registry simply serves as a repository for schemas to be referenced and stored. So this solves the issue of getting the consumer access to the schema the producer used without passing it directly to the consumer but we still have the issue of knowing which version of the schema to use.

To solve the concern of knowing which schema to use we need to mainly answer two questions. What schema should I use and what version of that schema should I use. To solve the problem of what schema to use the Confluent schema registry introduces the concept of subjects. A subject represents a scope in which a schema can evolve. A subject has a name that can be derived in [various ways](https://docs.confluent.io/platform/current/schema-registry/fundamentals/serdes-develop/index.html#sr-schemas-subject-name-strategy) but the most common is the `TopicNameStrategy`. As the name suggests the `TopicNameStrategy` builds a subject name based on the name of the Kafka topic it will be sent on. In addition to this, it adds either "-value" or "-key" to the end of the subject name as you can have a different schema for the key and the value of a Kafka record. By the nature of the subject we can know which schema to reference.

As for which version of a schema was used for any particular message, that could technically change from one message to another. The schema registry will hold all versions and thus can provide all versions but it has no knowledge of Kafka or its dealings. To solve this problem the `KafkaAvroSerializer` simply writes that data at the beginning of each message. This sounds similar to our rejected solution above of simply passing the schema with each message but it does it with much less data usage. Each version of a schema in the schema registry is given an integer identifier and this is what is used to tell the consuming side which schema to use for deserialization. At a byte level, this is how it is structured. The first byte of the message is a [magic byte](https://en.wikipedia.org/wiki/List_of_file_signatures) marking this as an Avro serialized message. To save space this magic byte is a single byte of all zeros (0x0), without this at the beginning of the message the `KafkaAvroDeserializer` will refuse to read the message. Next to that is four bytes of the integer that is the schema ID from the schema registry that is being used. After this the regularly serialized Avro data is output. This means there is a "waste" of 5 bytes per message sent (or 10 if both the key and value are using Avro) on Kafka. This is a very reasonable tradeoff in my opinion.

{% image "./avro_bytes.png", "Avro bytes diagram" %}

Now let's put it all together. Step one is we request that the Kafka producer produce a message on a topic. This then passes the data that was given to the producer to the `KafkaAvroSerializer`. The serializer takes the data along with the subject name strategy and information about the message and determines what the subject will be. Perhaps we are sending a message on the "test-avro" topic and we are serializing its value. If we are using the `TopicNameStrategy` we end up with "test-avro-value" as our subject. The serializer then queries a local cache it has to determine if it already knows the schema ID of that schema. If it's not in the cache we then send the schema along with the subject to the schema registry and query to find the schema ID. This can also be configured to register a schema in this step if you would like. The schema ID is returned and cached at which point the producer will no longer ever need to go to the schema registry for the ID until the cache is cleared. The producer then starts writing the message by setting the magic byte and then the schema ID. After this, it serializes the data as normal and puts those bytes after the schema ID bytes. The serializer can pass the data back to Kafka which sends the message.

{% image "./avro_sequence_producer.png", "Sequence diagram for producer" %}

On the receiving side, a lot is similar. The deserializer determines the subject based on the same information as above. It then makes sure the magic byte is present and retrieves the schema ID. It then checks its local cache to determine if it already has that schema pulled from the schema registry. If not, it requests that schema from the registry and stores it in its cache. It then grabs the writer schema that is now has retrieved from the schema registry and the reader schema which it has locally and provides both of these with the remaining data from the message. Avro then iterates through the data and deserializes it. The object is then returned out of the deserializer and the Kafka consumer client provides it to the caller.

{% image "./avro_sequence_consumer.png", "Sequence diagram of consumer" %}

Do you need to know how this works to use the `KafkaAvroSerializer`? Not at all. This system does a lot of work to be invisible to you and "just work". That said I think there is great value in knowing how these internals work. For one, debugging becomes much easier. I have had numerous times where I have had my deserializer complain that there was a missing magic byte. Before understanding this that error was much more confusing. The day that I felt like I truly grokked subjects was the first day that I stepped through this code, understood the purpose they were playing, and how they were useful to me. I've also had someone complain that they didn't understand why they needed to provide the topic that was being produced to when using the `KafkaAvroSerailizer` (not required when you use it embedded in the `KafkaProducer` but if used "raw" this becomes apparent). Once you understand what it needs to do with determining the subject it becomes quite apparent where the need comes from. Understanding how things work and developing the [mechanical sympathy](https://wa.aws.amazon.com/wellarchitected/2020-07-02T19-33-23/wat.concept.mechanical-sympathy.en.html) can be a superpower in all aspects of development in my opinion.